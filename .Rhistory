test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 2500)
randomForest(train.feat, train.lab, test.feat, test.lab, ntree = 10 )
randomForest(train.feat, train.lab, test.feat, test.lab, ntree = 1 )
randomForest(train.feat, train.lab, test.feat, test.lab, ntree = 100 )
if(!require("randomForest")) install.packages("randomForest")
rf <- randomForest(train.feat, train.lab, test.feat, test.lab, ntree = 100)
library("randomForest")
predict(rf,test.feat)
rf <- randomForest(train.feat, train.lab, test.feat, test.lab, ntree = 100)
rf
predict(rf,test.feat)
class(rf)
predict(rf , test.feat)
class(rf)
rf <- randomForest(train.feat, train.lab, ntree = 100)
predict(rf , test.feat)
train.lab
predict(rf , test.feat, type = "class")
sign(predict(rf , test.feat, type = "class"))
pred <- sign(predict(rf , test.feat, type = "class"))
err.rf <- sum(pred != test.lab) / length(test.lab)
err.rf
test <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
test <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 1200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 2500)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
iter <- 50
w <- rep(1/length(train.lab), length(train.lab))
err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, err = err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
iter <- 100
train <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 1200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
iter <- 100
w <- rep(1/length(train.lab), length(train.lab))
err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, err = err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
head(train.feat)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
head(train.feat)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
debug(get.data)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
ada.booster <- function(feat, lab, w, alpha.pred, iter = 10,
diagnostics = FALSE, err.rate = c()) {
tree = rpart(formula = lab~., data = feat,
weights = w, method = "class", control = c(maxdepth = 1) )
pred = ifelse(predict(tree, type = "class") == "-1", -1, 1)
epsilon = (function() sum(w * ifelse(pred == lab, 0, 1))) ()
alpha   = (function(x) 0.5 * log((1 - x) / x)) (epsilon)
w       = w * exp((alpha * ifelse(pred != lab, 1, -1)))
alpha.pred = alpha.pred + ifelse(pred == "-1", -1, 1) * alpha
if(diagnostics == TRUE) err.rate <- c(err.rate,
sum(sign(alpha.pred) != lab) / length(lab))
#Check if all weights are 0 i.e data is prefectly seperated
zero.weights = (max(w) == 0 & min (w) == 0)
if(iter == 1 | zero.weights) {
if(zero.weights) print("Data is prefectly seperated. Quitting")
if(diagnostics != TRUE) return(sign(alpha.pred))
else return (err.rate)
}
else {
ada.booster(feat, lab, w, alpha.pred, iter - 1, diagnostics, err.rate)
}
}
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
label = label[index])
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
iter <- 100
w <- rep(1/length(train.lab), length(train.lab))
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, err = err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
mu1 <- c(15, 19)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
undebug(ada.booster)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 19)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
train.feat <- train[[1]]
function(n1 = 100, n2 = 100, mu1, mu2, sd1, sd2, rhoXY, seed = 1000) {
set.seed(seed)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
}
undebug(ada.booster)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 19)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
function(n1 = 100, n2 = 100, mu1, mu2, sd1, sd2, rhoXY, seed = 1000) {
}
undebug(get.data)
undebug(get.data)
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 19)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
iter <- 100
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.rates
err.df <- data.frame(iter = 1:iter, err = err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 1200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
draw.plot(train.feat, train.lab)
draw.plot(test.feat, test.lab)
iter <- 100
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, err = err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
ad.err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
rf.err.rates <- sapply(X = 1:iter, FUN = function(v) {
rf <- randomForest(train.feat, train.lab, ntree = v)
pred <- sign(predict(rf , test.feat, type = "class"))
sum(pred != test.lab) / length(test.lab)
})
warning()
warnings()
rf.err.rates
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates, rand.forest =rf.err.rates)
if(!require("reshape2")) install.packages("reshape2")
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Type", value.name = "Error")
melt.error
names(melt.error)
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm", value.name = "Error")
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) + geom_point() + geom_line()
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates)
ggplot(err.df, aes(x = iter, y = err)) + geom_point() + geom_line()
err.df <- data.frame(iter = 1:iter, "Ada Boost Error" = ad.err.rates)
ggplot(err.df, aes(x = iter, y = "Ada Boost Error")) + geom_point() + geom_line()
iter <- 100
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
ad.err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, "Ada Boost Error" = ad.err.rates)
ggplot(err.df, aes(x = iter, y = "Ada Boost Error")) + geom_point() + geom_line()
err.df <- data.frame(iter = 1:iter, Ada_Boost_Error = ad.err.rates)
ggplot(err.df, aes(x = iter, y = Ada_Boost_Error)) + geom_point() + geom_line()
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(test.lab), length(test.lab))
w
ad.err.rates <- ada.booster(test.feat, test.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, Ada_Boost_Error = ad.err.rates)
ad.err.rates <- ada.booster(test.feat, test.lab, w, alpha.pred, iter, diagnostics = TRUE)
rf.err.rates <- sapply(X = 1:iter, FUN = function(v) {
rf <- randomForest(train.feat, train.lab, ntree = v)
pred <- sign(predict(rf , test.feat, type = "class"))
sum(pred != test.lab) / length(test.lab)
})
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates, rand.forest =rf.err.rates)
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm", value.name = "Error")
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) + geom_point() + geom_line()
draw.plot(train.feat, train.lab) +ggtitle("Train Data")
if(!require("grid")) install.packages("grid")
plot1 <- draw.plot(train.feat, train.lab) + ggtitle("Train Data")
plot2 <- draw.plot(test.feat, test.lab) + ggtitle("Test Data")
plot1 <- draw.plot(train.feat, train.lab) + ggtitle("Train Data")
plot2 <- draw.plot(test.feat, test.lab) + ggtitle("Test Data")
pushViewport(viewport(layout = grid.layout(1, 2)))
print(plot1, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(plot2, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
plot1 <- draw.plot(train.feat, train.lab) + ggtitle("Train Data")
plot2 <- draw.plot(test.feat, test.lab) + ggtitle("Test Data")
pushViewport(viewport(layout = grid.layout(1, 2)))
print(plot1, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(plot2, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
iter <- 100
#ADA BOOST
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
ad.err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, Ada_Boost_Error = ad.err.rates)
ggplot(err.df, aes(x = iter, y = Ada_Boost_Error)) + geom_point() + geom_line() +
ggtitle("Error rates with test data")
?rpart.control
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
bagging = bagging(train.feat ~., train.feat, mfinal=20,
control=rpart.control(maxdepth=v))
predict.bagging(bagging,newdata=test.feat)$error
})
if(!require("adabag")) install.packages("adabag")
if(!require("adabag")) install.packages("adabag")
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
bagging = bagging(train.feat ~., train.feat, mfinal=20,
control=rpart.control(maxdepth=v))
predict.bagging(bagging,newdata=test.feat)$error
})
v = 1
bagging = bagging(train.feat ~., train.feat, mfinal=20,
control=rpart.control(maxdepth=v))
bagging = bagging(train.lab ~., train.feat, mfinal=20,
control=rpart.control(maxdepth=v))
>bagging()
>bagging
?bagging
bag.train <- cbind(train.lab, train.feat)
bag.train
head(bag.train)
bagging = bagging(train.lab ~., bag.train, mfinal=20,
control=rpart.control(maxdepth=v))
bagging = bagging(train.lab ~., bag.train, mfinal=v)
names(bag.train)
bagging = bagging(train.lab ~ V1 + V2, bag.train, mfinal=v)
bag.train <- cbind(factor(train.lab), train.feat)
bag.train
names(bag.train)
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
}
}}}
bagging = bagging(train.lab ~ V1 + V2, bag.train, mfinal=v)
names(bag.train)
bag.train <- cbind(train.lab = factor(train.lab), train.feat)
bagging = bagging(train.lab ~ V1 + V2, bag.train, mfinal=v)
predict.bagging(bagging,newdata=test.feat)$error
predict.bagging(bagging,newdata=test.feat)
?predict.bagging
bagging = bagging(train.lab ~ V1 + V2, bag.train, mfinal=v)
test.feat
predict.bagging(object = bagging, newdata= test.feat)
})
iris[-sub,]
head(iris)
bag.test <- cbind(test.lab = factor(test.lab), test.feat)
predict.bagging(object = bagging, newdata= bag.test)
sub <- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
iris.bagging <- bagging(Species ~ ., data=iris[sub,], mfinal=10)
iris.bagging
iris.predbagging<- predict.bagging(iris.bagging, newdata=iris[-sub,])
pred = predict.bagging(object = bagging, newdata= test.feat)
bagging = bagging(train.lab ~., bag.train, mfinal=v)
pred = predict.bagging(object = bagging, newdata= test.feat)
pred = predict.bagging(bagging, test.feat)
class(iris[-sub,])
bag.test <- cbind(test.lab = factor(test.lab), test.feat)
bag.train <- cbind(lab = factor(train.lab), train.feat)
bag.test <- cbind(lab = factor(test.lab), test.feat)
bagging = bagging(train.lab ~., bag.train, mfinal=v)
bagging = bagging(lab ~., bag.train, mfinal=v)
pred = predict.bagging(bagging, test.feat)
names(bag.train)
names(bag.test)
bag = bagging(lab ~., bag.train, mfinal=v)
pred = predict.bagging(bag, test.feat)
test.feat
pred = predict.bagging(bag, bag.test)
pred = predict.bagging(bag, bag.test)$error
pred
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
bag = bagging(lab ~., bag.train, mfinal=v)
pred = predict.bagging(bag, bag.test)$error
})
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
bag = bagging(lab ~., bag.train, mfinal=v)
err = predict.bagging(bag, bag.test)$error
err
})
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) + geom_point() + geom_line()
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates,
rand.forest =rf.err.rates, bagging = bag.err.rates)
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm",
value.name = "Error")
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) + geom_point() + geom_line()
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) +
geom_point() +
geom_line() +
scale_color_manual(values = c("#ece7f2", "#a6bddb", "#2b8cbe"))
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm",
value.name = "Error")
names(melt.error)
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) +
geom_point() +
geom_line() +
scale_color_manual(values = c("#ece7f2", "#a6bddb", "#2b8cbe"))
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) +
geom_point() +
geom_line() +
scale_color_manual(values = c("#1f78b4", "#33a02c", "#ff7f00"))
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates,
rand.forest =rf.err.rates, bagging = bag.err.rates)
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm",
value.name = "Error")
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) +
geom_point() +
geom_line() +
scale_color_manual(values = c("#1f78b4", "#33a02c", "#ff7f00")) +
ggtitle("Test Error Comparison")
write.csv(x = err.df, file = "test_err.csv", row.names = FALSE)
if(!require("assertthat")) install.packages("assertthat")
if(!require("rpart")) install.packages("rpart")
if(!require("dplyr")) install.packages("dplyr")
if(!require("mvtnorm")) install.packages("mvtnorm")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("randomForest")) install.packages("randomForest")
if(!require("reshape2")) install.packages("reshape2")
if(!require("grid")) install.packages("grid")
get.sigmaXY <- function(rhoXY, sdX, sdY){
cov <- rhoXY * sdX *sdY
matrix(c(sdX^2, cov, cov, sdY^2), nrow = 2)
}
get.data <- function(n1 = 100, n2 = 100, mu1, mu2, sd1, sd2, rhoXY, seed = 1000) {
set.seed(seed)
features1 <- rmvnorm(n1, mean = c(mu1[1], mu1[2]),
sigma = get.sigmaXY(rhoXY[1], sd1[1], sd1[2]))
features2 <- rmvnorm(n2, mean = c(mu2[1], mu2[2]),
sigma = get.sigmaXY(rhoXY[2], sd2[1], sd2[2]))
features <- rbind(features1, features2)
label <- c(rep(-1, n1), rep(1, n2))
index <- sample(1:(n1+n2))
result <- list(features = as.data.frame(rbind(features1, features2))[index,],
label = label[index])
return(result)
}
draw.plot <- function(feat, lab) {
data <- data.frame(feat, factor(lab))
names(data) <- c("X1", "X2", "lab")
ggplot(data, aes(x = X1, y = X2, fill = lab, color = lab)) +
geom_point(alpha = 0.5, size = 3) +
scale_color_manual(values = c("#AD0909", "#202D8F"))
}
#Create dataset
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)
train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 200)
train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]
ada.booster <- function(feat, lab, w, alpha.pred, iter = 10,
diagnostics = FALSE, err.rate = c()) {
#Predict labels at stump level 1
tree = rpart(formula = lab~., data = feat,
weights = w, method = "class", control = c(maxdepth = 1) )
pred = ifelse(predict(tree, type = "class") == "-1", -1, 1)
#recalculate weights
epsilon = (function() sum(w * ifelse(pred == lab, 0, 1))) ()
alpha   = (function(x) 0.5 * log((1 - x) / x)) (epsilon)
w       = w * exp((alpha * ifelse(pred != lab, 1, -1)))
#calculate alpha * pred for iter
alpha.pred = alpha.pred + ifelse(pred == "-1", -1, 1) * alpha
#check diagnostics of function to decide output
if(diagnostics == TRUE) err.rate <- c(err.rate,
sum(sign(alpha.pred) != lab) / length(lab))
#Check if all weights are 0 i.e data is prefectly seperated
zero.weights = (max(w) == 0 & min (w) == 0)
if(iter == 1 | zero.weights) {
if(zero.weights) print("Data is prefectly seperated. Quitting")
if(diagnostics != TRUE) return(sign(alpha.pred))
else return (err.rate)
}
else {
ada.booster(feat, lab, w, alpha.pred, iter - 1, diagnostics, err.rate)
}
}
plot1 <- draw.plot(train.feat, train.lab) + ggtitle("Train Data")
plot2 <- draw.plot(test.feat, test.lab) + ggtitle("Test Data")
pushViewport(viewport(layout = grid.layout(1, 2)))
print(plot1, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(plot2, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
iter <- 100
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
ad.err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)
err.df <- data.frame(iter = 1:iter, Ada_Boost_Error = ad.err.rates)
ggplot(err.df, aes(x = iter, y = Ada_Boost_Error)) + geom_point() + geom_line() +
ggtitle("Error rates with test data")
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(test.lab), length(test.lab))
ad.err.rates <- ada.booster(test.feat, test.lab, w, alpha.pred, iter, diagnostics = TRUE)
rf.err.rates <- sapply(X = 1:iter, FUN = function(v) {
rf <- randomForest(train.feat, train.lab, ntree = v)
pred <- sign(predict(rf , test.feat, type = "class"))
sum(pred != test.lab) / length(test.lab)
})
#BAGGING
bag.train <- cbind(lab = factor(train.lab), train.feat)
bag.test <- cbind(lab = factor(test.lab), test.feat)
bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
bag = bagging(lab ~., bag.train, mfinal=v)
err = predict.bagging(bag, bag.test)$error
err
})
library("class")
source("code/library.R")
setwd("c:/Users/Rishabh/Documents/GitHub/kForest/")
source("code/library.R")
load("data/train.RData")
train.pred <- knn.cv(train = train, cl = cover_type, k = 1)
train.pred
table(pred = train.pred, true = cover_type)
error <- sum(train.pred != cover_type) / length(train.pred)
error
k.range <- c(1, 3, 5)
pred <- sapply(X = k.range, FUN = function(x) knn.cv(train = train, cl = cover_type, k = x))
str(pred)
error <- apply(X = pred, MARGIN = 2, FUN =  function(x) sum(x != cover_type) / length(x))
error
pred.test <- knn(train = train, test = test, cl = cover_type, k = 1)
load("data/test.RData")
setwd("c:/Users/Rishabh/Documents/GitHub/kForest/")
source("code/library.R")
load("data/train.RData")
load("data/test.RData")
pred <- sapply(X = k.range, FUN = function(x) knn.cv(train = train, cl = cover_type, k = x))
k.range <- c(1, 3, 5)
pred <- sapply(X = k.range, FUN = function(x) knn.cv(train = train, cl = cover_type, k = x))
error <- apply(X = pred, MARGIN = 2, FUN =  function(x) sum(x != cover_type) / length(x))
pred.test <- knn(train = train, test = test, cl = cover_type, k = 1)
knn.pred.test <- data.frame(Id = id, Cover_Type = cover_type)
write.csv(knn.pred.test, "data/knn/knn1_pred.csv")
knn.pred.test <- data.frame(Id = id, Cover_Type = pred.test)
write.csv(knn.pred.test, "data/knn/knn1_pred.csv")
write.csv(knn.pred.test, "data/knn/knn1_pred.csv", row.names = FALSE)
